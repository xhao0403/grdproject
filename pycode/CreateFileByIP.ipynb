{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function\n",
    "\n",
    "# 获取csv文件的列名\n",
    "def getcols(filename):\n",
    "    data = pd.read_csv(filename, nrows = 2)\n",
    "    #print(data.columns)\n",
    "    return data.columns\n",
    "\n",
    "# 获取文件行数\n",
    "def getrows(filename):\n",
    "    cols = getcols(filename)\n",
    "    col = cols[0]\n",
    "    data = pd.read_csv(filename, usecols=[col])\n",
    "    return data.shape[0]\n",
    "\n",
    "# 数据预处理- step1: 删除无法用于计算的列与含空值、负值、inf值的行\n",
    "def preprocess2017(data):\n",
    "    labels = data[' Label']\n",
    "    # 删除非数值型数据，这些数据暂时不用于聚类处理，但不代表这些数据没用\n",
    "    dropset = ['Flow ID', ' Source IP', ' Source Port',\n",
    "       ' Destination IP', ' Destination Port', ' Protocol', ' Timestamp',\n",
    "           ' Fwd Header Length.1', ' Label']\n",
    "    # 删除含有大量负数的列\n",
    "    drop_nega = [' Fwd Header Length', 'Init_Win_bytes_forward',\n",
    "           ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
    "       ' min_seg_size_forward']\n",
    "    # 删除数据中可能出现的非数值型数据列\n",
    "    drop_ifexist = ['External IP','SimillarHTTP', ' Inbound','Unnamed: 0']\n",
    "    \n",
    "    # 如果有list中的columns则删除\n",
    "    data.drop(data.columns[data.columns.isin(dropset+drop_nega+drop_ifexist)], axis = 1, inplace=True)\n",
    "    \n",
    "    data.dropna(how='any', inplace=True)\n",
    "    print(len(data))\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    # 将所有数据类型转成float型\n",
    "    data = data.astype('float')\n",
    "    \n",
    "    # 删除数据中inf的值 (需要连续执行两次，不知道原因，但连续执行后就可删除数据中的inf)\n",
    "    idx_tuple = np.where(data.max(axis=1).values == np.inf)\n",
    "    idx = list(idx_tuple[0])\n",
    "   # print(idx)\n",
    "    data.drop(axis=0, index = idx, inplace = True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    labels.drop(axis=0, index = idx, inplace = True)\n",
    "    labels.reset_index(inplace=True, drop=True)\n",
    "   # print(len(data))\n",
    "    idx_tuple = np.where(data.max(axis=1).values == np.inf)\n",
    "    idx = list(idx_tuple[0])\n",
    "    data.drop(axis=0, index = idx, inplace = True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    labels.drop(axis=0, index = idx, inplace = True)\n",
    "    labels.reset_index(inplace=True, drop=True)\n",
    "   # print(len(data))\n",
    "\n",
    "    # 删除数据中含负数的数据条数\n",
    "    idx_tuple = np.where(data.min(axis=1).values < 0)\n",
    "    idx = list(idx_tuple[0])\n",
    "    data.drop(axis=0, index = idx, inplace = True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    labels.drop(axis=0, index = idx, inplace = True)\n",
    "    labels.reset_index(inplace=True, drop=True)\n",
    "   # print(len(data))\n",
    "    return data,labels\n",
    "\n",
    "def preprocess2019(data, cols, labelnum, label):\n",
    "    data.dropna(how='any', inplace=True)\n",
    "    \n",
    "    # print(data.columns)\n",
    "    data.drop(cols[0:2],axis = 1, inplace = True)\n",
    "    \n",
    "    # 删除非数值型数据，这些数据暂时不用于聚类处理，但不代表这些数据没用\n",
    "    dropset = ['Flow ID', ' Source IP', ' Source Port',\n",
    "       ' Destination IP', ' Destination Port', ' Protocol', ' Timestamp',\n",
    "           ' Fwd Header Length.1', 'SimillarHTTP', ' Inbound']\n",
    "    \n",
    "    # 删除含有大量负数的列\n",
    "    drop_nega = [' Fwd Header Length', 'Init_Win_bytes_forward',\n",
    "           ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
    "       ' min_seg_size_forward']\n",
    "    data.drop(dropset, axis=1,inplace=True)\n",
    "    data.drop(drop_nega, axis=1,inplace=True)\n",
    "    \n",
    "    # 将标签替换为数字\n",
    "    data.replace(to_replace=label, value=labelnum, inplace=True)\n",
    "    \n",
    "    # 将所有数据类型转成float型\n",
    "    data = data.astype('float')\n",
    "    \n",
    "    # 删除数据中inf的值\n",
    "    idx_tuple = np.where(data.max(axis=1).values == np.inf)\n",
    "    idx = list(idx_tuple[0])\n",
    "#    print(len(idx))\n",
    "    data.drop(axis=0, index = idx, inplace = True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    idx_tuple = np.where(data.max(axis=1).values == np.inf)\n",
    "    idx = list(idx_tuple[0])\n",
    "#    print(len(idx))\n",
    "    data.drop(axis=0, index = idx, inplace = True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # 删除数据中含负数的数据条数\n",
    "    idx_tuple = np.where(data.min(axis=1).values < 0)\n",
    "    idx = list(idx_tuple[0])\n",
    "    data.drop(axis=0, index = idx, inplace = True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件夹'/mnt/hgfs/linuxfile/2017/'中读取数据文件\n",
    "filedir = '/mnt/hgfs/linuxfile/2017/'\n",
    "files = os.listdir(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
       " 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
       " 'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
       " 'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
       " 'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
       " 'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
       " 'Wednesday-workingHours.pcap_ISCX.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 受害者IP\n",
    "DDos_victim_IP = ['192.168.10.50'] # 128834\n",
    "PortScan_victim_IP = ['192.168.10.50'] # 160716\n",
    "Botnet_victim_IP = ['192.168.10.15', '192.168.10.9', '192.168.10.14', '192.168.10.5', '192.168.10.8'] # 16029 \n",
    "WebAttack_victim_IP = ['192.168.10.50'] # 4230\n",
    "Infiltration_victim_IP = ['192.168.10.8', '192.168.10.25'] # 15943\n",
    "BruteForce_victim_IP = ['192.168.10.50'] # 4188\n",
    "DDos2_victim_IP = ['192.168.10.50','205.174.165.66','192.168.10.51'] # 23926\n",
    "ips = [DDos_victim_IP, PortScan_victim_IP, Botnet_victim_IP, Infiltration_victim_IP, WebAttack_victim_IP, BruteForce_victim_IP, DDos2_victim_IP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "victims = ['DDos_victim', 'PortScan_victim', 'Botnet_victim', 'WebAttack_victim', 'Infiltration_victim', 'BruteForce_victim', 'DDos2_victim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuhao/miniconda3/envs/grdenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3166: DtypeWarning: Columns (85) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/xuhao/miniconda3/envs/grdenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3166: DtypeWarning: Columns (0,1,3,6,84) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ips_len = len(ips)\n",
    "for i in range(ips_len):\n",
    "    data = pd.read_csv(filedir+files[i])\n",
    "    data = data[data[' Destination IP'].isin(ips[i])]\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    data.to_csv('/mnt/hgfs/linuxfile/victim2017/'+victims[i]+'_data.csv', mode='w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件夹'/mnt/hgfs/linuxfile/2019/0112/'中读取数据文件\n",
    "filedir = '/mnt/hgfs/linuxfile/2019/0112/'\n",
    "files = os.listdir(filedir)\n",
    "victim2019_IP = ['192.168.50.1'] # 受害者ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuhao/miniconda3/envs/grdenv/lib/python3.7/site-packages/IPython/core/async_helpers.py:68: DtypeWarning: Columns (85) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  coro.send(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 5th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 6th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 7th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 8th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 9th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 10th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 11th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 12th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 13th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 14th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 15th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_DNS_victim_data.csv\n",
      "DrDoS_DNS.csv' 16th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_LDAP_victim_data.csv\n",
      "DrDoS_LDAP.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_LDAP_victim_data.csv\n",
      "DrDoS_LDAP.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_LDAP_victim_data.csv\n",
      "DrDoS_LDAP.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_LDAP_victim_data.csv\n",
      "DrDoS_LDAP.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_LDAP_victim_data.csv\n",
      "DrDoS_LDAP.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_LDAP_victim_data.csv\n",
      "DrDoS_LDAP.csv' 5th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_LDAP_victim_data.csv\n",
      "DrDoS_LDAP.csv' 6th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_LDAP_victim_data.csv\n",
      "DrDoS_LDAP.csv' 7th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 5th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 6th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 7th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 8th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 9th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 10th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 11th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 12th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 13th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 14th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_MSSQL_victim_data.csv\n",
      "DrDoS_MSSQL.csv' 15th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 5th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 6th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 7th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 8th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 9th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 10th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 11th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 12th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NetBIOS_victim_data.csv\n",
      "DrDoS_NetBIOS.csv' 13th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NTP_victim_data.csv\n",
      "DrDoS_NTP.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NTP_victim_data.csv\n",
      "DrDoS_NTP.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NTP_victim_data.csv\n",
      "DrDoS_NTP.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NTP_victim_data.csv\n",
      "DrDoS_NTP.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_NTP_victim_data.csv\n",
      "DrDoS_NTP.csv' 4th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 5th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 6th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 7th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 8th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 9th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 10th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 11th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 12th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 13th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 14th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 15th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 16th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SNMP_victim_data.csv\n",
      "DrDoS_SNMP.csv' 17th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 5th loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 6th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 7th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_SSDP_victim_data.csv\n",
      "DrDoS_SSDP.csv' 8th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 5th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 6th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 7th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 8th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 9th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/DrDoS_UDP_victim_data.csv\n",
      "DrDoS_UDP.csv' 10th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/Syn_victim_data.csv\n",
      "Syn.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/Syn_victim_data.csv\n",
      "Syn.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/Syn_victim_data.csv\n",
      "Syn.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/Syn_victim_data.csv\n",
      "Syn.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/Syn_victim_data.csv\n",
      "Syn.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/Syn_victim_data.csv\n",
      "Syn.csv' 5th loop\n",
      "Interation is stopped\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 0th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 1th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 2th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 3th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 4th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 5th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 6th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 7th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 8th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 9th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 10th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 11th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 12th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 13th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 14th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 15th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 16th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 17th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 18th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 19th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 20th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 21th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 22th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 23th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 24th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 25th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 26th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 27th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 28th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 29th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 30th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 31th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 32th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 33th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 34th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 35th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 36th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 37th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 38th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 39th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 40th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 41th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 42th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 43th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 44th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 45th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 46th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 47th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 48th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 49th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 50th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 51th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 52th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 53th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 54th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 55th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 56th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 57th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 58th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 59th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 60th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 61th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 62th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 63th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 64th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 65th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 66th loop\n",
      "add data to /mnt/hgfs/linuxfile/victim2019/TFTP_victim_data.csv\n",
      "TFTP.csv' 67th loop\n",
      "Interation is stopped\n"
     ]
    }
   ],
   "source": [
    "files_num = len(files)\n",
    "\n",
    "victims = [''] * len(files)\n",
    "for i in range(len(victims)):\n",
    "    victims[i] = files[i][:-4] + '_victim'\n",
    "    \n",
    "    \n",
    "for i in range(files_num):\n",
    "    index = 0\n",
    "    loop = True\n",
    "    chunks = []\n",
    "    chunksize = 300000\n",
    "    data = pd.read_csv(filedir+files[i], iterator = True)\n",
    "    while loop:\n",
    "        try:\n",
    "            chunk = data.get_chunk(chunksize)\n",
    "            tdata = chunk[chunk[' Destination IP'].isin(victim2019_IP)]\n",
    "            if tdata.empty == False:\n",
    "                if os.path.exists('/mnt/hgfs/linuxfile/victim2019/'+victims[i]+'_data.csv') == False:\n",
    "                     tdata.to_csv('/mnt/hgfs/linuxfile/victim2019/'+victims[i]+'_data.csv', mode='a+')\n",
    "                else:\n",
    "                    tdata.to_csv('/mnt/hgfs/linuxfile/victim2019/'+victims[i]+'_data.csv', mode='a+', header = False)\n",
    "                print(\"add data to \"+'/mnt/hgfs/linuxfile/victim2019/'+victims[i]+'_data.csv')\n",
    "            print(files[i]+'\\' '+str(index)+\"th loop\")\n",
    "            index += 1\n",
    "        except StopIteration:\n",
    "            loop = False\n",
    "            print(\"Interation is stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers:\n",
      "\n",
      "read_csv(filepath_or_buffer: Union[ForwardRef('PathLike[str]'), str, IO[~T], io.RawIOBase, io.BufferedIOBase, io.TextIOBase, _io.TextIOWrapper, mmap.mmap], sep=<object object at 0x7f30580bb060>, delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: str = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options: Union[Dict[str, Any], NoneType] = None)\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : bool, default False\n",
      "        If the parsed data only contains one column then return a Series.\n",
      "    prefix : str, optional\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    mangle_dupe_cols : bool, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python'}, optional\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True.\n",
      "    false_values : list, optional\n",
      "        Values to consider as False.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "        'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "        specify ``date_parser`` to be a partially-applied\n",
      "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "        :ref:`io.csv.mixed_timezones` for more.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "        .. versionadded:: 0.25.0\n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and\n",
      "        `filepath_or_buffer` is path-like, then detect compression from the\n",
      "        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
      "        decompression). If using 'zip', the ZIP file must contain only one data\n",
      "        file to be read in. Set to None for no decompression.\n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
      "           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
      "           This behavior was previously only the case for ``engine=\"python\"``.\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    error_bad_lines : bool, default True\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
      "        returned.\n",
      "    warn_bad_lines : bool, default True\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or 'high' for the ordinary converter,\n",
      "        'legacy' for the original lower precision pandas converter, and\n",
      "        'round_trip' for the round-trip converter.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc., if using a URL that will\n",
      "        be parsed by ``fsspec``, e.g., starting \"s3://\", \"gcs://\". An error\n",
      "        will be raised if providing this argument with a non-fsspec URL.\n",
      "        See the fsspec and backend storage implementation docs for the set of\n",
      "        allowed keys and values.\n",
      "    \n",
      "        .. versionadded:: 1.2\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextParser\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:grdenv]",
   "language": "python",
   "name": "conda-env-grdenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
